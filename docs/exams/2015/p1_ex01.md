---
id: ex-2015-01
year: 2015
exam: parcial 1
tags:
 - complejidad
---

- (a) (5 puntos) Como medida de similitud (o diferencia) entre dos documentos dados, representados como ficheros de texto, compañías como Google utilizan el concepto de “distancia” (o su contrario, “proximidad”) entre dos documentos, según el algoritmo que se describe a continuación. 

Se descomponen los dos ficheros de texto en palabras y, para cada uno de los dos ficheros, se construye un vector ordenado con tantos componentes como palabras diferentes hay entre los dos ficheros, siendo el valor de cada elemento el número de veces que la palabra en cuestión se repite en el fichero de texto dado. En especial, si una determinada palabra existe en un fichero pero no en el otro, el valor correspondiente para el segundo vector, 
es cero. Por ejemplo, 

| Palabra    | d1 | d2 |
| ---------- | -- | -- |
| árbol      | 5  | 7  |
| barco      | 2  | 0  |
| …          | …  | …  |
| zapatiesta | 0  | 3  |

La palabra ‘árbol’ aparece 5 veces en el fichero 1 y 7 veces en el 2. La palabra ‘barco’ no aparece en el fichero 2. La palabra ‘zapatiesta’ no aparece en el fichero 1.

A continuación se realiza el producto escalar de los dos vectores que, una vez dividido por el producto del número total de palabras que hay en cada uno de los dos textos, nos da como resultado la medida de proximidad entre los dos documentos dados. 

Suponiendo que los dos documentos a comparar tienen una longitud similar, digamos N caracteres, y que la longitud máxima de una palabra en español es K, se pide dar la complejidad del algoritmo descrito en términos de N, a base de justificar la complejidad de cada una de las partes de las que se compone (el algoritmo). 

NOTA: Si no puede justificar la complejidad del algoritmo de ordenación, considere que es O(NlogN).

??? note "Mostrar solución"
    La extracción de las palabras se realiza iterando dentro de un simple bucle a lo largo de cada uno de los dos textos, lo cual da una complejidad lineal, es decir, O(N). 

    Cada vez que en el bucle anterior se encuentra una palabra, se consultan y actualizan dos HashMap, cuyas operaciones son en todo caso de orden constante, y se hacen un número máximo de ellas finito (y pequeño) por cada palabra encontrada, por lo que la fase anterior no ve modificada su complejidad de O(N). 

    A continuación han de construirse los dos vectores ordenados a partir de los valores que están desordenados en los HashMap, lo cual nos dicen que tiene una complejidad de O(N*logN). 

    Por último, el cálculo del producto escalar es otro bucle simple, repetido tantas veces como palabras diferentes hay, de una multiplicación (complejidad constante) más una suma (también complejidad constante), lo cual da una complejidad para el producto escalar de O(N). La división posterior, siendo una sola y de complejidad constante, no cambia la complejidad de esta fase.  
    
    Asi pues, la complejidad total será, sumando las de cada una de sus tres fases, 
    
    O(N) + O(N*logN) + O(N), lo cual se reduce, simplemente, a O(N*logN).
